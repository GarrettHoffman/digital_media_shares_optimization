{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to MongoDB collection\n",
    "client = pymongo.MongoClient()\n",
    "db = client.mashable\n",
    "collection = client.mashable.articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pull article content from MongoDB\n",
    "content = []\n",
    "for doc in collection.find({}, {'_id': 0, 'content': 1}):\n",
    "    content.append(doc['content'].encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39494"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of documents is equal to expected\n",
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define stop words to exclude from LDA topic modeling\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "content_no_punc = [\"\".join(char for char in text\n",
    "                           if char not in string.punctuation) \n",
    "                   for text in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove stopwords and tokenize\n",
    "documents = [[word.decode('utf-8')\n",
    "              for word in text.lower().split() \n",
    "              if word.decode('utf-8') not in stop] \n",
    "              for text in content_no_punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define lemmatizer\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatize vocabularly\n",
    "documents = [[lmtzr.lemmatize(token) for token in doc]\n",
    "              for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for doc in documents:\n",
    "     for token in doc:\n",
    "            frequency[token] += 1\n",
    "\n",
    "documents = [[token for token in doc if frequency[token] > 1]\n",
    "              for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "# store the dictionary, for future reference\n",
    "dictionary.save('mashable_LDA_dictionary.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load dictionary\n",
    "dictionary = corpora.Dictionary.load('mashable_LDA_dictionary.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create corpus for model\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store to disk, for later use\n",
    "corpora.MmCorpus.serialize('mashable_LDA_corpara.mm', corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load corpus\n",
    "corpus = corpora.MmCorpus('mashable_LDA_corpara.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train LDA model\n",
    "# alpha and eta are hyperparameters that affect sparsity of the \n",
    "# document-topic (theta) and topic-word (lambda) distributions. \n",
    "# Both default to a symmetric 1.0/num_topics prior. Setting to 'auto'\n",
    "# will learns an asymmetric prior directly from your data.\n",
    "\n",
    "lda = models.LdaModel(corpus,\n",
    "               id2word = dictionary,\n",
    "               alpha = 'auto',\n",
    "               eta = 'auto',\n",
    "               num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "lda.save('mashable.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "lda = models.LdaModel.load('mashable.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.011*device + 0.008*also + 0.008*phone + 0.007*\\u2014 + 0.006*iphone + 0.006*new + 0.006*screen + 0.006*one + 0.005*camera + 0.005*apple + 0.005*like + 0.005*tablet + 0.004*price + 0.004*see + 0.004*car + 0.004*ipad + 0.004*display + 0.004*design + 0.004*samsung + 0.003*watch'),\n",
       " (1,\n",
       "  u'0.020*image + 0.010*photo + 0.008*also + 0.006*see + 0.006*make + 0.006*courtesy + 0.005*like + 0.004*food + 0.004*add + 0.004*look + 0.004*dog + 0.004*youre + 0.004*one + 0.004*take + 0.004*get + 0.004*holiday + 0.003*say + 0.003*day + 0.003*cat + 0.003*gift'),\n",
       " (2,\n",
       "  u'0.028*game + 0.009*player + 0.009*team + 0.008*\\u2014 + 0.007*world + 0.007*one + 0.005*sport + 0.005*get + 0.005*also + 0.005*play + 0.004*sony + 0.004*see + 0.004*time + 0.004*could + 0.004*new + 0.004*fan + 0.004*cup + 0.004*thats + 0.004*video + 0.004*last'),\n",
       " (3,\n",
       "  u'0.012*said + 0.006*government + 0.006*u + 0.005*also + 0.005*\\u2014 + 0.005*new + 0.005*state + 0.004*job + 0.004*medium + 0.004*people + 0.004*security + 0.003*report + 0.003*president + 0.003*company + 0.003*would + 0.003*law + 0.003*group + 0.003*time + 0.003*comment + 0.003*country'),\n",
       " (4,\n",
       "  u'0.009*said + 0.006*\\u2014 + 0.006*also + 0.005*2014 + 0.004*image + 0.004*flight + 0.004*storm + 0.004*area + 0.004*see + 0.004*year + 0.004*ukraine + 0.003*one + 0.003*city + 0.003*water + 0.003*according + 0.003*space + 0.003*air + 0.003*new + 0.003*u + 0.003*state'),\n",
       " (5,\n",
       "  u'0.008*video + 0.008*\\u2014 + 0.007*show + 0.007*also + 0.007*see + 0.005*new + 0.005*one + 0.005*story + 0.005*time + 0.005*film + 0.005*like + 0.004*movie + 0.004*something + 0.004*comment + 0.004*share + 0.004*song + 0.004*get + 0.004*star + 0.004*episode + 0.004*season'),\n",
       " (6,\n",
       "  u'0.016*user + 0.015*app + 0.011*google + 0.009*also + 0.009*facebook + 0.008*new + 0.007*apps + 0.007*twitter + 0.007*see + 0.006*\\u2014 + 0.005*share + 0.005*io + 0.005*feature + 0.005*image + 0.005*add + 0.005*like + 0.005*social + 0.005*comment + 0.004*site + 0.004*photo'),\n",
       " (7,\n",
       "  u'0.025*2014 + 0.018*\\u2014 + 0.008*police + 0.004*also + 0.004*protester + 0.004*year + 0.004*day + 0.004*2013 + 0.004*ferguson + 0.003*woman + 0.003*image + 0.003*december + 0.003*november + 0.003*protest + 0.003*city + 0.003*october + 0.003*new + 0.003*world + 0.003*see + 0.003*brown'),\n",
       " (8,\n",
       "  u'0.009*\\u2014 + 0.007*people + 0.006*one + 0.006*said + 0.005*say + 0.005*like + 0.005*also + 0.004*time + 0.004*year + 0.004*would + 0.003*see + 0.003*something + 0.003*new + 0.003*could + 0.003*get + 0.003*story + 0.003*way + 0.003*thing + 0.003*world + 0.003*make'),\n",
       " (9,\n",
       "  u'0.016*company + 0.010*apple + 0.008*year + 0.008*million + 0.007*also + 0.007*new + 0.006*\\u2014 + 0.006*said + 0.005*business + 0.005*service + 0.005*share + 0.005*see + 0.004*product + 0.004*customer + 0.004*ad + 0.004*time + 0.004*comment + 0.004*billion + 0.004*amazon + 0.004*market')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test LDA feature generation\n",
    "test_doc = collection.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Associated Press is the latest news organization to experiment with trying to make money from Twitter by using its feed to advertise for other companies.  The AP announced Monday that it will share sponsored tweets from Samsung throughout this week for the International CES taking place in Las Vegas. The news service will let Samsung post two tweets per day to the AP\\'s Twitter account, which has more than 1.5 million users, and each of these tweets will be labeled \"SPONSORED TWEETS.\" This marks the first time that the AP has sold advertising on its Twitter feed, and the company says it spent months developing guidelines to pave the way for this and other new media business models.  For this particular promotion, Samsung will provide the sponsored tweets and non-editorial staff at the AP will handle the publishing side. In this way, the company hopes to maintain a clear dividing line between its editorial and advertising operations on Twitter. \"We are thrilled to be taking this next step in social media,\" said Lou Ferrara, the AP managing editor overseeing its social media efforts, in a statement. \"As an industry, we must be looking for new ways to develop revenues while providing good experiences for advertisers and consumers. At the same time, advertisers and audiences expect AP to do that without compromising its core mission of breaking news.\"  Other publishers have dabbled in Twitter ads, including The Atlantic, National Journal, The Times-Picayune and BreakingNews.com. Image courtesy of Flickr, nan palmero'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull content from Mongo Doc\n",
    "test_doc_content = test_doc['content'].encode('utf8')\n",
    "test_doc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Associated Press is the latest news organization to experiment with trying to make money from Twitter by using its feed to advertise for other companies  The AP announced Monday that it will share sponsored tweets from Samsung throughout this week for the International CES taking place in Las Vegas The news service will let Samsung post two tweets per day to the APs Twitter account which has more than 15 million users and each of these tweets will be labeled SPONSORED TWEETS This marks the first time that the AP has sold advertising on its Twitter feed and the company says it spent months developing guidelines to pave the way for this and other new media business models  For this particular promotion Samsung will provide the sponsored tweets and noneditorial staff at the AP will handle the publishing side In this way the company hopes to maintain a clear dividing line between its editorial and advertising operations on Twitter We are thrilled to be taking this next step in social media said Lou Ferrara the AP managing editor overseeing its social media efforts in a statement As an industry we must be looking for new ways to develop revenues while providing good experiences for advertisers and consumers At the same time advertisers and audiences expect AP to do that without compromising its core mission of breaking news  Other publishers have dabbled in Twitter ads including The Atlantic National Journal The TimesPicayune and BreakingNewscom Image courtesy of Flickr nan palmero'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation\n",
    "test_doc_content = \"\".join(char for char \n",
    "                           in test_doc_content \n",
    "                           if char \n",
    "                           not in string.punctuation)\n",
    "test_doc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'associated',\n",
       " u'press',\n",
       " u'latest',\n",
       " u'news',\n",
       " u'organization',\n",
       " u'experiment',\n",
       " u'trying',\n",
       " u'make',\n",
       " u'money',\n",
       " u'twitter',\n",
       " u'using',\n",
       " u'feed',\n",
       " u'advertise',\n",
       " u'companies',\n",
       " u'ap',\n",
       " u'announced',\n",
       " u'monday',\n",
       " u'share',\n",
       " u'sponsored',\n",
       " u'tweets',\n",
       " u'samsung',\n",
       " u'throughout',\n",
       " u'week',\n",
       " u'international',\n",
       " u'ces',\n",
       " u'taking',\n",
       " u'place',\n",
       " u'las',\n",
       " u'vegas',\n",
       " u'news',\n",
       " u'service',\n",
       " u'let',\n",
       " u'samsung',\n",
       " u'post',\n",
       " u'two',\n",
       " u'tweets',\n",
       " u'per',\n",
       " u'day',\n",
       " u'aps',\n",
       " u'twitter',\n",
       " u'account',\n",
       " u'15',\n",
       " u'million',\n",
       " u'users',\n",
       " u'tweets',\n",
       " u'labeled',\n",
       " u'sponsored',\n",
       " u'tweets',\n",
       " u'marks',\n",
       " u'first',\n",
       " u'time',\n",
       " u'ap',\n",
       " u'sold',\n",
       " u'advertising',\n",
       " u'twitter',\n",
       " u'feed',\n",
       " u'company',\n",
       " u'says',\n",
       " u'spent',\n",
       " u'months',\n",
       " u'developing',\n",
       " u'guidelines',\n",
       " u'pave',\n",
       " u'way',\n",
       " u'new',\n",
       " u'media',\n",
       " u'business',\n",
       " u'models',\n",
       " u'particular',\n",
       " u'promotion',\n",
       " u'samsung',\n",
       " u'provide',\n",
       " u'sponsored',\n",
       " u'tweets',\n",
       " u'noneditorial',\n",
       " u'staff',\n",
       " u'ap',\n",
       " u'handle',\n",
       " u'publishing',\n",
       " u'side',\n",
       " u'way',\n",
       " u'company',\n",
       " u'hopes',\n",
       " u'maintain',\n",
       " u'clear',\n",
       " u'dividing',\n",
       " u'line',\n",
       " u'editorial',\n",
       " u'advertising',\n",
       " u'operations',\n",
       " u'twitter',\n",
       " u'thrilled',\n",
       " u'taking',\n",
       " u'next',\n",
       " u'step',\n",
       " u'social',\n",
       " u'media',\n",
       " u'said',\n",
       " u'lou',\n",
       " u'ferrara',\n",
       " u'ap',\n",
       " u'managing',\n",
       " u'editor',\n",
       " u'overseeing',\n",
       " u'social',\n",
       " u'media',\n",
       " u'efforts',\n",
       " u'statement',\n",
       " u'industry',\n",
       " u'must',\n",
       " u'looking',\n",
       " u'new',\n",
       " u'ways',\n",
       " u'develop',\n",
       " u'revenues',\n",
       " u'providing',\n",
       " u'good',\n",
       " u'experiences',\n",
       " u'advertisers',\n",
       " u'consumers',\n",
       " u'time',\n",
       " u'advertisers',\n",
       " u'audiences',\n",
       " u'expect',\n",
       " u'ap',\n",
       " u'without',\n",
       " u'compromising',\n",
       " u'core',\n",
       " u'mission',\n",
       " u'breaking',\n",
       " u'news',\n",
       " u'publishers',\n",
       " u'dabbled',\n",
       " u'twitter',\n",
       " u'ads',\n",
       " u'including',\n",
       " u'atlantic',\n",
       " u'national',\n",
       " u'journal',\n",
       " u'timespicayune',\n",
       " u'breakingnewscom',\n",
       " u'image',\n",
       " u'courtesy',\n",
       " u'flickr',\n",
       " u'nan',\n",
       " u'palmero']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords and tokenize\n",
    "test_doc_content = [word.decode('utf-8')\n",
    "                    for word in test_doc_content.lower().split() \n",
    "                    if word.decode('utf-8') not in stop] \n",
    "test_doc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'associated',\n",
       " u'press',\n",
       " u'latest',\n",
       " u'news',\n",
       " u'organization',\n",
       " u'experiment',\n",
       " u'trying',\n",
       " u'make',\n",
       " u'money',\n",
       " u'twitter',\n",
       " u'using',\n",
       " u'feed',\n",
       " u'advertise',\n",
       " u'company',\n",
       " u'ap',\n",
       " u'announced',\n",
       " u'monday',\n",
       " u'share',\n",
       " u'sponsored',\n",
       " u'tweet',\n",
       " u'samsung',\n",
       " u'throughout',\n",
       " u'week',\n",
       " u'international',\n",
       " u'ce',\n",
       " u'taking',\n",
       " u'place',\n",
       " u'la',\n",
       " u'vega',\n",
       " u'news',\n",
       " u'service',\n",
       " u'let',\n",
       " u'samsung',\n",
       " u'post',\n",
       " u'two',\n",
       " u'tweet',\n",
       " u'per',\n",
       " u'day',\n",
       " u'aps',\n",
       " u'twitter',\n",
       " u'account',\n",
       " u'15',\n",
       " u'million',\n",
       " u'user',\n",
       " u'tweet',\n",
       " u'labeled',\n",
       " u'sponsored',\n",
       " u'tweet',\n",
       " u'mark',\n",
       " u'first',\n",
       " u'time',\n",
       " u'ap',\n",
       " u'sold',\n",
       " u'advertising',\n",
       " u'twitter',\n",
       " u'feed',\n",
       " u'company',\n",
       " u'say',\n",
       " u'spent',\n",
       " u'month',\n",
       " u'developing',\n",
       " u'guideline',\n",
       " u'pave',\n",
       " u'way',\n",
       " u'new',\n",
       " u'medium',\n",
       " u'business',\n",
       " u'model',\n",
       " u'particular',\n",
       " u'promotion',\n",
       " u'samsung',\n",
       " u'provide',\n",
       " u'sponsored',\n",
       " u'tweet',\n",
       " u'noneditorial',\n",
       " u'staff',\n",
       " u'ap',\n",
       " u'handle',\n",
       " u'publishing',\n",
       " u'side',\n",
       " u'way',\n",
       " u'company',\n",
       " u'hope',\n",
       " u'maintain',\n",
       " u'clear',\n",
       " u'dividing',\n",
       " u'line',\n",
       " u'editorial',\n",
       " u'advertising',\n",
       " u'operation',\n",
       " u'twitter',\n",
       " u'thrilled',\n",
       " u'taking',\n",
       " u'next',\n",
       " u'step',\n",
       " u'social',\n",
       " u'medium',\n",
       " u'said',\n",
       " u'lou',\n",
       " u'ferrara',\n",
       " u'ap',\n",
       " u'managing',\n",
       " u'editor',\n",
       " u'overseeing',\n",
       " u'social',\n",
       " u'medium',\n",
       " u'effort',\n",
       " u'statement',\n",
       " u'industry',\n",
       " u'must',\n",
       " u'looking',\n",
       " u'new',\n",
       " u'way',\n",
       " u'develop',\n",
       " u'revenue',\n",
       " u'providing',\n",
       " u'good',\n",
       " u'experience',\n",
       " u'advertiser',\n",
       " u'consumer',\n",
       " u'time',\n",
       " u'advertiser',\n",
       " u'audience',\n",
       " u'expect',\n",
       " u'ap',\n",
       " u'without',\n",
       " u'compromising',\n",
       " u'core',\n",
       " u'mission',\n",
       " u'breaking',\n",
       " u'news',\n",
       " u'publisher',\n",
       " u'dabbled',\n",
       " u'twitter',\n",
       " u'ad',\n",
       " u'including',\n",
       " u'atlantic',\n",
       " u'national',\n",
       " u'journal',\n",
       " u'timespicayune',\n",
       " u'breakingnewscom',\n",
       " u'image',\n",
       " u'courtesy',\n",
       " u'flickr',\n",
       " u'nan',\n",
       " u'palmero']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize vocabularly\n",
    "test_doc_content = [lmtzr.lemmatize(token) for token in test_doc_content]\n",
    "test_doc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.17629512356881796),\n",
       " (6, 0.19677207416550102),\n",
       " (7, 0.01741500406999354),\n",
       " (9, 0.60377688639372074)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get topic distribution\n",
    "lda[dictionary.doc2bow(test_doc_content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 6, 7, 9}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get set of topics\n",
    "test_doc_topics = {topic for topic,prob in lda[dictionary.doc2bow(test_doc_content)]}\n",
    "test_doc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0,\n",
       " 2: 0,\n",
       " 3: 0.17628670319338244,\n",
       " 4: 0,\n",
       " 5: 0,\n",
       " 6: 0.19677729199618429,\n",
       " 7: 0.017397160437935408,\n",
       " 8: 0,\n",
       " 9: 0.60378533152714498}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get LDA topic dictionary\n",
    "test_LDA_topics = dict()\n",
    "\n",
    "for i in range(10):\n",
    "    if i in test_doc_topics: \n",
    "        for topic,prob in lda[dictionary.doc2bow(test_doc_content)]:\n",
    "            if topic == i:\n",
    "                test_LDA_topics[i] = prob\n",
    "    else:\n",
    "        test_LDA_topics[i] = 0\n",
    "        \n",
    "test_LDA_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define function to get LDA features\n",
    "\n",
    "def get_lda_features(doc):\n",
    "    \n",
    "    \"\"\"\n",
    "    Pull document from MongoDB collection of Mashable Articles \n",
    "    and generate LDA topic probabilities.\n",
    "    \n",
    "    Arguments:\n",
    "    Doc -- MongoDB Document\n",
    "    \n",
    "    Output:\n",
    "    Stores LDA topic probability results in Mongo DB for Document\n",
    "    \"\"\"\n",
    "    \n",
    "    # pull content from Mongo Doc\n",
    "    content = doc['content'].encode('utf8')\n",
    "    \n",
    "    # remove punctuation\n",
    "    content = \"\".join(char for char \n",
    "                      in content \n",
    "                      if char \n",
    "                      not in string.punctuation)\n",
    "    \n",
    "    # remove stopwords and tokenize\n",
    "    content = [word.decode('utf-8')\n",
    "               for word in content.lower().split() \n",
    "               if word.decode('utf-8') not in stop]\n",
    "    \n",
    "    # lemmatize vocabularly\n",
    "    content = [lmtzr.lemmatize(token) \n",
    "               for token in content]\n",
    "    \n",
    "    # get LDA features for Model\n",
    "    topic_probs = lda[dictionary.doc2bow(content)]\n",
    "    topics = {}\n",
    "    topics = {topic for (topic,prob) in topic_probs}\n",
    "    LDA_topics = dict()\n",
    "    for i in range(10):\n",
    "        if i in topics: \n",
    "            for (topic,prob) in topic_probs:\n",
    "                if topic == i:\n",
    "                    LDA_topics[i] = prob\n",
    "        else:\n",
    "            LDA_topics[i] = 0\n",
    "    \n",
    "    collection.update_one({\"_id\": doc[\"_id\"]}, \n",
    "                          {\"$set\": {\"LDA_0_prob\": LDA_topics[0], \n",
    "                                    \"LDA_1_prob\": LDA_topics[1],\n",
    "                                    \"LDA_2_prob\": LDA_topics[2], \n",
    "                                    \"LDA_3_prob\": LDA_topics[3], \n",
    "                                    \"LDA_4_prob\": LDA_topics[4],\n",
    "                                    \"LDA_5_prob\": LDA_topics[5], \n",
    "                                    \"LDA_6_prob\": LDA_topics[6],\n",
    "                                    \"LDA_7_prob\": LDA_topics[7],\n",
    "                                    \"LDA_8_prob\": LDA_topics[8],\n",
    "                                    \"LDA_9_prob\": LDA_topics[9]}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
