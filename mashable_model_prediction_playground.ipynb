{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "from textstat.textstat import textstat\n",
    "import statsmodels.api as sm\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load Poisson Regression Model for Share Count Predictions\n",
    "with open('pois_regress.pkl', 'rb') as f:\n",
    "    pois_reg = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load Random Forest Classification Model for Viarality Probability Predictions\n",
    "with open('rf_class.pkl', 'rb') as f:\n",
    "    RF_class = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load topic model processing tools and LDA model\n",
    "stop = stopwords.words('english')\n",
    "lmtzr = WordNetLemmatizer()\n",
    "dictionary = corpora.Dictionary.load('mashable_LDA_dictionary.dict')\n",
    "corpus = corpora.MmCorpus('mashable_LDA_corpara.mm')\n",
    "lda = models.LdaModel.load('mashable.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headline = 'Facebook rolls out suicide prevention tools in Australia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = \"Many of us confess more about our feelings on Facebook than we might ever say face-to-face with another person. Recognising it could have a role to play in preventing self-harm, Facebook has come up with a number of suicide prevention tools to help people who may be depressed. The program launched in the U.S. in February, and is now rolling out in Australia, a Facebook spokesperson confirmed to Mashable Australia.  In the U.S., Facebook partnered with local mental health initiatives including Forefront, Now Matters Now and the National Suicide Prevention Lifeline to develop the language around the initiative. In Australia, Facebook is working with BeyondBlue and Headspace in a collaboration announced Friday.  Keeping people safe is our most important responsibility on Facebook, the spokesperson said.  If someone thinks another person is considering suicide based on their Facebook posts, they are urged to call emergency services, but also to report the material to Facebook.  The company said it has teams working around the clock to review the reports. Depending on the seriousness of the threat, those assessing the posts can encourage the author to speak to a mental health expert through a private pop-up message or to reach out to a friend, or even provide advice on how to come to terms with their feelings.  We're also providing new resources and support to the person who flagged the troubling post, the spokesperson said, including options for them to call or message their distressed friend letting them know they care, or reaching out to another friend or a trained professional at a suicide hotline for support.  Globally there are multiple millions of users all going through these same problems, Chris Tanti, Headspace CEO told Fairfax Media about the tools. People can be notified and help can be provided just about anywhere in Australia, which is fantastic.\"\n",
    "content = content.encode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = 'Australia, Facebook, Mental Health, Social Media'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_published = 'Sunday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channel = 'Social Media'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_imgs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data dictionary and add fields tied to inputs\n",
    "def create_metadata_fields(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Define dictionary of data to be input to mashable models for prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # add values for content metadata\n",
    "    data['num_imgs'] = num_imgs\n",
    "    data['num_tags'] = len(tags.replace(' ','').split(\",\"))\n",
    "    data['num_videos'] = 0\n",
    "    \n",
    "    # add values for weekday published\n",
    "    if day_published == 'Monday':\n",
    "        data['weekday_is_monday'] = 1\n",
    "    else:\n",
    "        data['weekday_is_monday'] = 0\n",
    "        \n",
    "    if day_published == 'Tuesday':\n",
    "        data['weekday_is_tuesday'] = 1\n",
    "    else:\n",
    "        data['weekday_is_tuesday'] = 0\n",
    "        \n",
    "    if day_published == 'Wednesday':\n",
    "        data['weekday_is_wednesday'] = 1\n",
    "    else:\n",
    "        data['weekday_is_wednesday'] = 0\n",
    "        \n",
    "    if day_published == 'Thursday':\n",
    "        data['weekday_is_thursday'] = 1\n",
    "    else:\n",
    "        data['weekday_is_thursday'] = 0\n",
    "        \n",
    "    if day_published == 'Friday':\n",
    "        data['weekday_is_friday'] = 1\n",
    "    else:\n",
    "        data['weekday_is_friday'] = 0\n",
    "        \n",
    "    if day_published == 'Saturday':\n",
    "        data['weekday_is_saturday'] = 1\n",
    "    else:\n",
    "        data['weekday_is_saturday'] = 0\n",
    "        \n",
    "    if day_published == 'Sunday':\n",
    "        data['weekday_is_sunday'] = 1\n",
    "    else:\n",
    "        data['weekday_is_sunday'] = 0\n",
    "        \n",
    "    if day_published == 'Saturday' or day_published == 'Sunday':\n",
    "        data['is_weekend'] = 1\n",
    "    else:\n",
    "        data['is_weekend'] = 0\n",
    "    \n",
    "    # add values for channel\n",
    "    if channel == 'Business':\n",
    "        data['data_channel_is_bus'] = 1\n",
    "    else:\n",
    "        data['data_channel_is_bus'] = 0\n",
    "    \n",
    "    if channel == 'Entertainment':\n",
    "        data['data_channel_is_entertainment'] = 1\n",
    "    else:\n",
    "        data['data_channel_is_entertainment'] = 0\n",
    "    \n",
    "    if channel == 'Lifestyle':\n",
    "        data['data_channel_is_lifestyle'] = 1\n",
    "    else:\n",
    "        data['data_channel_is_lifestyle'] = 0\n",
    "        \n",
    "    if channel == 'Social Media':\n",
    "        data['data_channel_is_socmed'] = 1\n",
    "    else:\n",
    "        data['data_channel_is_socmed'] = 0\n",
    "    \n",
    "    if channel == 'Technology':\n",
    "        data['data_channel_is_tech'] = 1\n",
    "    else:\n",
    "        data['data_channel_is_tech'] = 0\n",
    "        \n",
    "    if channel == 'World':\n",
    "        data['data_channel_is_world'] = 1\n",
    "    else:\n",
    "        data['data_channel_is_world'] = 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [u'LDA_0_prob', u'LDA_1_prob', u'LDA_2_prob', u'LDA_3_prob',\n",
    "       u'LDA_4_prob', u'LDA_5_prob', u'LDA_6_prob', u'LDA_7_prob',\n",
    "       u'LDA_8_prob', u'LDA_9_prob', u'average_token_length_content',\n",
    "       u'average_token_length_title', u'avg_negative_polarity',\n",
    "       u'avg_positive_polarity', u'data_channel_is_bus',\n",
    "       u'data_channel_is_entertainment', u'data_channel_is_lifestyle',\n",
    "       u'data_channel_is_socmed', u'data_channel_is_tech',\n",
    "       u'data_channel_is_world', u'global_grade_level',\n",
    "       u'global_rate_negative_words', u'global_rate_positive_words',\n",
    "       u'global_reading_ease', u'global_sentiment_abs_polarity',\n",
    "       u'global_sentiment_polarity', u'global_subjectivity', u'is_weekend',\n",
    "       u'max_abs_polarity', u'max_negative_polarity', u'max_positive_polarity',\n",
    "       u'min_negative_polarity', u'min_positive_polarity', u'n_tokens_content',\n",
    "       u'n_tokens_title', u'num_imgs', u'num_tags', u'num_videos',\n",
    "       u'r_non_stop_unique_tokens', u'r_non_stop_words', u'r_unique_tokens',\n",
    "       u'rate_negative_words', u'rate_positive_words',\n",
    "       u'title_sentiment_abs_polarity', u'title_sentiment_polarity',\n",
    "       u'title_subjectivity', u'weekday_is_friday',\n",
    "       u'weekday_is_monday', u'weekday_is_saturday', u'weekday_is_sunday',\n",
    "       u'weekday_is_thursday', u'weekday_is_tuesday', u'weekday_is_wednesday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(index=index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA_0_prob</th>\n",
       "      <th>LDA_1_prob</th>\n",
       "      <th>LDA_2_prob</th>\n",
       "      <th>LDA_3_prob</th>\n",
       "      <th>LDA_4_prob</th>\n",
       "      <th>LDA_5_prob</th>\n",
       "      <th>LDA_6_prob</th>\n",
       "      <th>LDA_7_prob</th>\n",
       "      <th>LDA_8_prob</th>\n",
       "      <th>LDA_9_prob</th>\n",
       "      <th>...</th>\n",
       "      <th>title_sentiment_abs_polarity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  LDA_0_prob LDA_1_prob LDA_2_prob LDA_3_prob LDA_4_prob LDA_5_prob  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "  LDA_6_prob LDA_7_prob LDA_8_prob LDA_9_prob         ...           \\\n",
       "0        NaN        NaN        NaN        NaN         ...            \n",
       "\n",
       "  title_sentiment_abs_polarity title_sentiment_polarity title_subjectivity  \\\n",
       "0                          NaN                      NaN                NaN   \n",
       "\n",
       "  weekday_is_friday weekday_is_monday weekday_is_saturday weekday_is_sunday  \\\n",
       "0               NaN               NaN                 NaN               NaN   \n",
       "\n",
       "  weekday_is_thursday weekday_is_tuesday weekday_is_wednesday  \n",
       "0                 NaN                NaN                  NaN  \n",
       "\n",
       "[1 rows x 53 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_metadata_fields(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA_0_prob</th>\n",
       "      <th>LDA_1_prob</th>\n",
       "      <th>LDA_2_prob</th>\n",
       "      <th>LDA_3_prob</th>\n",
       "      <th>LDA_4_prob</th>\n",
       "      <th>LDA_5_prob</th>\n",
       "      <th>LDA_6_prob</th>\n",
       "      <th>LDA_7_prob</th>\n",
       "      <th>LDA_8_prob</th>\n",
       "      <th>LDA_9_prob</th>\n",
       "      <th>...</th>\n",
       "      <th>title_sentiment_abs_polarity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  LDA_0_prob LDA_1_prob LDA_2_prob LDA_3_prob LDA_4_prob LDA_5_prob  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "  LDA_6_prob LDA_7_prob LDA_8_prob LDA_9_prob          ...           \\\n",
       "0        NaN        NaN        NaN        NaN          ...            \n",
       "\n",
       "  title_sentiment_abs_polarity title_sentiment_polarity title_subjectivity  \\\n",
       "0                          NaN                      NaN                NaN   \n",
       "\n",
       "  weekday_is_friday  weekday_is_monday  weekday_is_saturday  \\\n",
       "0                 0                  0                    0   \n",
       "\n",
       "   weekday_is_sunday  weekday_is_thursday  weekday_is_tuesday  \\\n",
       "0                  1                    0                   0   \n",
       "\n",
       "   weekday_is_wednesday  \n",
       "0                     0  \n",
       "\n",
       "[1 rows x 53 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_NLP_features(data):\n",
    "\n",
    "    # generate headline features\n",
    "\n",
    "    # number of words in title\n",
    "    data['n_tokens_title'] = len(headline.split())\n",
    "\n",
    "    # subjectivity\n",
    "    data['title_subjectivity'] = TextBlob(headline).subjectivity\n",
    "\n",
    "    # polarity\n",
    "    data['title_sentiment_polarity'] = TextBlob(headline).polarity\n",
    "\n",
    "    # absolute value polarirty\n",
    "    data['title_sentiment_abs_polarity'] = abs(data['title_sentiment_polarity'])\n",
    "\n",
    "    # average word length\n",
    "    data['average_token_length_title'] = np.mean([len(w) for w \n",
    "                                          in \"\".join(c for c in headline \n",
    "                                                     if c not in string.punctuation).split()])\n",
    "\n",
    "    #generate content features\n",
    "\n",
    "    # number of words\n",
    "    data['n_tokens_content'] = len([w for w in content.split()])\n",
    "\n",
    "    # rate of unique words\n",
    "    data['r_unique_tokens'] = len(set([w.lower().decode('utf-8')\n",
    "                               for w \n",
    "                               in \"\".join(c for c in content \n",
    "                                          if c not in string.punctuation).split()]))/data['n_tokens_content']\n",
    "\n",
    "    # rate of non-stop word\n",
    "    data['r_non_stop_words'] = len([w.lower().decode('utf-8') \n",
    "                            for w in \"\".join(c for c in content \n",
    "                                             if c not in string.punctuation).split() \n",
    "                            if w.decode('utf-8') \n",
    "                            not in stop])/data['n_tokens_content']\n",
    "\n",
    "    # rate of unique non-stop word\n",
    "    data['r_non_stop_unique_tokens'] = len(set([w.lower().decode('utf-8') \n",
    "                               for w in \"\".join(c for c in content \n",
    "                                                if c not in string.punctuation).split() \n",
    "                               if w.decode('utf-8')\n",
    "                               not in stop]))/data['n_tokens_content']\n",
    "\n",
    "    # average word length\n",
    "    data['average_token_length_content'] = np.mean([len(w) for w \n",
    "                                            in \"\".join(c for c in content\n",
    "                                                       if c not in string.punctuation).split()])\n",
    "\n",
    "    # subjectivity\n",
    "    data['global_subjectivity'] = TextBlob(content.decode('utf-8')).subjectivity\n",
    "\n",
    "    # polarity\n",
    "    data['global_sentiment_polarity'] = TextBlob(content.decode('utf-8')).polarity\n",
    "\n",
    "    # absolute polarity\n",
    "    data['global_sentiment_abs_polarity'] = abs(data['global_sentiment_polarity'])\n",
    "\n",
    "    # get polarity by word\n",
    "    polarity_list = [(w.decode('utf-8'), TextBlob(w.decode('utf-8')).polarity) \n",
    "                             for w in \"\".join(c for c in content \n",
    "                                              if c not in string.punctuation).split()]\n",
    "\n",
    "    # global positive word rate\n",
    "    data['global_rate_positive_words'] = len([(w,p) \n",
    "                                      for (w,p) \n",
    "                                      in polarity_list \n",
    "                                      if p > 0])/len(polarity_list)\n",
    "\n",
    "    # global negative word rate\n",
    "    data['global_rate_negative_words'] = len([(w,p) \n",
    "                                      for (w,p) \n",
    "                                      in polarity_list \n",
    "                                      if p < 0])/len(polarity_list)\n",
    "\n",
    "    # positive word rate (among non-nuetral words)\n",
    "    if [(w,p) for (w,p) in polarity_list if p != 0]:\n",
    "        data['rate_positive_words'] = len([(w,p) \n",
    "                                   for (w,p) \n",
    "                                   in polarity_list \n",
    "                                   if p > 0])/len([(w,p) \n",
    "                                                   for (w,p) \n",
    "                                                   in polarity_list \n",
    "                                                   if p != 0])\n",
    "    else:\n",
    "        data['rate_positive_words'] = 0\n",
    "\n",
    "    # negative word rate (among non-nuetral words)\n",
    "    if [(w,p) for (w,p) in polarity_list if p != 0]:\n",
    "        data['rate_negative_words'] = len([(w,p) \n",
    "                                   for (w,p) \n",
    "                                   in polarity_list \n",
    "                                   if p < 0])/len([(w,p) \n",
    "                                                   for (w,p) \n",
    "                                                   in polarity_list \n",
    "                                                   if p != 0])\n",
    "\n",
    "    else:\n",
    "        data['rate_negative_words'] = 0 \n",
    "\n",
    "    # average polarity of positive words\n",
    "    if [p for (w,p) in polarity_list if p > 0]:\n",
    "        data['avg_positive_polarity'] = np.mean([p for (w,p) \n",
    "                                         in polarity_list \n",
    "                                         if p > 0])\n",
    "    else:\n",
    "        data['avg_positive_polarity'] = 0\n",
    "\n",
    "    # minimum polarity of positive words\n",
    "    if [p for (w,p) in polarity_list if p > 0]:\n",
    "        data['min_positive_polarity'] = min([p for (w,p) \n",
    "                                     in polarity_list \n",
    "                                     if p > 0])\n",
    "    else:\n",
    "        data['min_positive_polarity'] = 0\n",
    "\n",
    "    # maximum polarity of positive words\n",
    "    if [p for (w,p) in polarity_list if p > 0]:\n",
    "        data['max_positive_polarity'] = max([p for (w,p) \n",
    "                                     in polarity_list \n",
    "                                     if p > 0])\n",
    "    else: \n",
    "        data['max_positive_polarity'] = 0\n",
    "\n",
    "    # average polarity of negative words\n",
    "    if [p for (w,p) in polarity_list if p < 0]:\n",
    "        data['avg_negative_polarity'] = np.mean([p for (w,p) \n",
    "                                         in polarity_list \n",
    "                                         if p < 0])\n",
    "    else:\n",
    "        data['avg_negative_polarity'] = 0\n",
    "\n",
    "    # minimum polarity of negative words\n",
    "    if [p for (w,p) in polarity_list if p < 0]:\n",
    "        data['min_negative_polarity'] = min([p for (w,p) \n",
    "                                     in polarity_list \n",
    "                                     if p < 0])\n",
    "    else:\n",
    "        data['min_negative_polarity'] = 0\n",
    "\n",
    "    # maximum polarity of negative words\n",
    "    if [p for (w,p) in polarity_list if p < 0]:\n",
    "        data['max_negative_polarity'] = max([p for (w,p) \n",
    "                                 in polarity_list \n",
    "                                 if p < 0])\n",
    "    else:\n",
    "        data['max_negative_polarity'] = 0\n",
    "\n",
    "    # abs maximum polarity, sum of abs of max positive and abs of min negative polarity\n",
    "    data['max_abs_polarity'] = data['max_positive_polarity'] + abs(data['min_negative_polarity'])\n",
    "\n",
    "    # Flesch Reading Ease\n",
    "    data['global_reading_ease'] = textstat.flesch_reading_ease(content.decode('utf-8'))\n",
    "\n",
    "    # Flesch Kincaid Grade Level\n",
    "    data['global_grade_level'] = textstat.flesch_kincaid_grade(content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_NLP_features(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA_0_prob</th>\n",
       "      <th>LDA_1_prob</th>\n",
       "      <th>LDA_2_prob</th>\n",
       "      <th>LDA_3_prob</th>\n",
       "      <th>LDA_4_prob</th>\n",
       "      <th>LDA_5_prob</th>\n",
       "      <th>LDA_6_prob</th>\n",
       "      <th>LDA_7_prob</th>\n",
       "      <th>LDA_8_prob</th>\n",
       "      <th>LDA_9_prob</th>\n",
       "      <th>...</th>\n",
       "      <th>title_sentiment_abs_polarity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  LDA_0_prob LDA_1_prob LDA_2_prob LDA_3_prob LDA_4_prob LDA_5_prob  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "  LDA_6_prob LDA_7_prob LDA_8_prob LDA_9_prob          ...           \\\n",
       "0        NaN        NaN        NaN        NaN          ...            \n",
       "\n",
       "   title_sentiment_abs_polarity  title_sentiment_polarity  title_subjectivity  \\\n",
       "0                             0                         0                   0   \n",
       "\n",
       "   weekday_is_friday  weekday_is_monday  weekday_is_saturday  \\\n",
       "0                  0                  0                    0   \n",
       "\n",
       "   weekday_is_sunday  weekday_is_thursday  weekday_is_tuesday  \\\n",
       "0                  1                    0                   0   \n",
       "\n",
       "   weekday_is_wednesday  \n",
       "0                     0  \n",
       "\n",
       "[1 rows x 53 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polarity_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polarity_list = [(w.decode('utf-8'), TextBlob(w.decode('utf-8')).polarity) \n",
    "                  for w in \"\".join(c for c in content \n",
    "                                   if c not in string.punctuation).split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polarity_data['children'] = [{'word': w, 'polarity': abs(p), 'color': '#1A79BB'} if p>0 \n",
    "                 else {'word': w, 'polarity': abs(p), 'color': '#bb1a29'}\n",
    "                 for (w,p) in polarity_list if p != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': [{'color': '#1A79BB', 'polarity': 0.5, 'word': u'Many'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.5, 'word': u'more'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.4, 'word': u'confirmed'},\n",
       "  {'color': '#bb1a29', 'polarity': 0.1, 'word': u'mental'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.5, 'word': u'safe'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.5, 'word': u'most'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.4, 'word': u'important'},\n",
       "  {'color': '#bb1a29', 'polarity': 0.1, 'word': u'mental'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.13636363636363635, 'word': u'new'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.1, 'word': u'professional'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.4, 'word': u'fantastic'}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lda_features(data):\n",
    "\n",
    "    # remove punctuation\n",
    "    content_tmp = \"\".join(char for char \n",
    "                      in content \n",
    "                      if char \n",
    "                      not in string.punctuation)\n",
    "    \n",
    "    # remove stopwords and tokenize\n",
    "    content_tmp = [word.decode('utf-8')\n",
    "               for word in content_tmp.lower().split() \n",
    "               if word.decode('utf-8') not in stop]\n",
    "    \n",
    "    # lemmatize vocabularly\n",
    "    content_tmp = [lmtzr.lemmatize(token) \n",
    "               for token in content_tmp]\n",
    "    \n",
    "    # get LDA features for Model\n",
    "    topic_probs = lda[dictionary.doc2bow(content_tmp)]\n",
    "    topics = {}\n",
    "    topics = {topic for (topic,prob) in topic_probs}\n",
    "    LDA_topics = dict()\n",
    "    for i in range(10):\n",
    "        if i in topics: \n",
    "            for (topic,prob) in topic_probs:\n",
    "                if topic == i:\n",
    "                    LDA_topics[i] = prob\n",
    "        else:\n",
    "            LDA_topics[i] = 0\n",
    "    \n",
    "    data['LDA_0_prob'] = LDA_topics[0] \n",
    "    data['LDA_1_prob'] = LDA_topics[1]\n",
    "    data['LDA_2_prob'] = LDA_topics[2] \n",
    "    data['LDA_3_prob'] = LDA_topics[3] \n",
    "    data['LDA_4_prob'] = LDA_topics[4]\n",
    "    data['LDA_5_prob'] = LDA_topics[5] \n",
    "    data['LDA_6_prob'] = LDA_topics[6]\n",
    "    data['LDA_7_prob'] = LDA_topics[7]\n",
    "    data['LDA_8_prob'] = LDA_topics[8]\n",
    "    data['LDA_9_prob'] = LDA_topics[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_lda_features(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA_0_prob</th>\n",
       "      <th>LDA_1_prob</th>\n",
       "      <th>LDA_2_prob</th>\n",
       "      <th>LDA_3_prob</th>\n",
       "      <th>LDA_4_prob</th>\n",
       "      <th>LDA_5_prob</th>\n",
       "      <th>LDA_6_prob</th>\n",
       "      <th>LDA_7_prob</th>\n",
       "      <th>LDA_8_prob</th>\n",
       "      <th>LDA_9_prob</th>\n",
       "      <th>...</th>\n",
       "      <th>title_sentiment_abs_polarity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.395167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031147</td>\n",
       "      <td>0.250184</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0</td>\n",
       "      <td>0.297654</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LDA_0_prob  LDA_1_prob  LDA_2_prob  LDA_3_prob  LDA_4_prob  LDA_5_prob  \\\n",
       "0           0           0    0.395167           0           0    0.031147   \n",
       "\n",
       "   LDA_6_prob  LDA_7_prob  LDA_8_prob  LDA_9_prob          ...           \\\n",
       "0    0.250184    0.022346           0    0.297654          ...            \n",
       "\n",
       "   title_sentiment_abs_polarity  title_sentiment_polarity  title_subjectivity  \\\n",
       "0                             0                         0                   0   \n",
       "\n",
       "   weekday_is_friday  weekday_is_monday  weekday_is_saturday  \\\n",
       "0                  0                  0                    0   \n",
       "\n",
       "   weekday_is_sunday  weekday_is_thursday  weekday_is_tuesday  \\\n",
       "0                  1                    0                   0   \n",
       "\n",
       "   weekday_is_wednesday  \n",
       "0                     0  \n",
       "\n",
       "[1 rows x 53 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_metadata_fields(data)\n",
    "create_NLP_features(data)\n",
    "create_lda_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LDA_0_prob': 0,\n",
       " 'LDA_1_prob': 0,\n",
       " 'LDA_2_prob': 0.39520162159397149,\n",
       " 'LDA_3_prob': 0,\n",
       " 'LDA_4_prob': 0,\n",
       " 'LDA_5_prob': 0.031060534226415401,\n",
       " 'LDA_6_prob': 0.25022956502977983,\n",
       " 'LDA_7_prob': 0.022314919231455951,\n",
       " 'LDA_8_prob': 0,\n",
       " 'LDA_9_prob': 0.29769134615999893,\n",
       " 'average_token_length_content': 5.0657894736842106,\n",
       " 'average_token_length_title': 6.125,\n",
       " 'avg_negative_polarity': -0.10000000000000001,\n",
       " 'avg_positive_polarity': 0.38181818181818178,\n",
       " 'data_channel_is_bus': 0,\n",
       " 'data_channel_is_entertainment': 0,\n",
       " 'data_channel_is_lifestyle': 0,\n",
       " 'data_channel_is_socmed': 1,\n",
       " 'data_channel_is_tech': 0,\n",
       " 'data_channel_is_world': 0,\n",
       " 'global_grade_level': 11.5,\n",
       " 'global_rate_negative_words': 0.006578947368421052,\n",
       " 'global_rate_positive_words': 0.029605263157894735,\n",
       " 'global_reading_ease': 50.16,\n",
       " 'global_sentiment_abs_polarity': 0.20227272727272724,\n",
       " 'global_sentiment_polarity': 0.20227272727272724,\n",
       " 'global_subjectivity': 0.3971590909090909,\n",
       " 'is_weekend': 1,\n",
       " 'max_abs_polarity': 0.6,\n",
       " 'max_negative_polarity': -0.1,\n",
       " 'max_positive_polarity': 0.5,\n",
       " 'min_negative_polarity': -0.1,\n",
       " 'min_positive_polarity': 0.1,\n",
       " 'n_tokens_content': 304,\n",
       " 'n_tokens_title': 8,\n",
       " 'num_imgs': 2,\n",
       " 'num_tags': 4,\n",
       " 'num_videos': 0,\n",
       " 'r_non_stop_unique_tokens': 0.4276315789473684,\n",
       " 'r_non_stop_words': 0.5789473684210527,\n",
       " 'r_unique_tokens': 0.5526315789473685,\n",
       " 'rate_negative_words': 0.18181818181818182,\n",
       " 'rate_positive_words': 0.8181818181818182,\n",
       " 'title_sentiment_abs_polarity': 0.0,\n",
       " 'title_sentiment_polarity': 0.0,\n",
       " 'title_subjectivity': 0.0,\n",
       " 'weekday_is_friday': 0,\n",
       " 'weekday_is_monday': 0,\n",
       " 'weekday_is_saturday': 0,\n",
       " 'weekday_is_sunday': 1,\n",
       " 'weekday_is_thursday': 0,\n",
       " 'weekday_is_tuesday': 0,\n",
       " 'weekday_is_wednesday': 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(pois_reg.predict(sm.add_constant(data_df))[0],-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(RF_class.predict_proba(sm.add_constant(data_df))[0][1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "data['est_shares'] = round(pois_reg.predict(sm.add_constant(data_df))[0],-2)\n",
    "data['est_prob'] = round(RF_class.predict_proba(sm.add_constant(data_df))[0][1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LDA_0_prob': 0,\n",
       " 'LDA_1_prob': 0,\n",
       " 'LDA_2_prob': 0.39520162159397149,\n",
       " 'LDA_3_prob': 0,\n",
       " 'LDA_4_prob': 0,\n",
       " 'LDA_5_prob': 0.031060534226415401,\n",
       " 'LDA_6_prob': 0.25022956502977983,\n",
       " 'LDA_7_prob': 0.022314919231455951,\n",
       " 'LDA_8_prob': 0,\n",
       " 'LDA_9_prob': 0.29769134615999893,\n",
       " 'average_token_length_content': 5.0657894736842106,\n",
       " 'average_token_length_title': 6.125,\n",
       " 'avg_negative_polarity': -0.10000000000000001,\n",
       " 'avg_positive_polarity': 0.38181818181818178,\n",
       " 'data_channel_is_bus': 0,\n",
       " 'data_channel_is_entertainment': 0,\n",
       " 'data_channel_is_lifestyle': 0,\n",
       " 'data_channel_is_socmed': 1,\n",
       " 'data_channel_is_tech': 0,\n",
       " 'data_channel_is_world': 0,\n",
       " 'est_prob': 0.54,\n",
       " 'est_shares': 3200.0,\n",
       " 'global_grade_level': 11.5,\n",
       " 'global_rate_negative_words': 0.006578947368421052,\n",
       " 'global_rate_positive_words': 0.029605263157894735,\n",
       " 'global_reading_ease': 50.16,\n",
       " 'global_sentiment_abs_polarity': 0.20227272727272724,\n",
       " 'global_sentiment_polarity': 0.20227272727272724,\n",
       " 'global_subjectivity': 0.3971590909090909,\n",
       " 'is_weekend': 1,\n",
       " 'max_abs_polarity': 0.6,\n",
       " 'max_negative_polarity': -0.1,\n",
       " 'max_positive_polarity': 0.5,\n",
       " 'min_negative_polarity': -0.1,\n",
       " 'min_positive_polarity': 0.1,\n",
       " 'n_tokens_content': 304,\n",
       " 'n_tokens_title': 8,\n",
       " 'num_imgs': 2,\n",
       " 'num_tags': 4,\n",
       " 'num_videos': 0,\n",
       " 'r_non_stop_unique_tokens': 0.4276315789473684,\n",
       " 'r_non_stop_words': 0.5789473684210527,\n",
       " 'r_unique_tokens': 0.5526315789473685,\n",
       " 'rate_negative_words': 0.18181818181818182,\n",
       " 'rate_positive_words': 0.8181818181818182,\n",
       " 'title_sentiment_abs_polarity': 0.0,\n",
       " 'title_sentiment_polarity': 0.0,\n",
       " 'title_subjectivity': 0.0,\n",
       " 'weekday_is_friday': 0,\n",
       " 'weekday_is_monday': 0,\n",
       " 'weekday_is_saturday': 0,\n",
       " 'weekday_is_sunday': 1,\n",
       " 'weekday_is_thursday': 0,\n",
       " 'weekday_is_tuesday': 0,\n",
       " 'weekday_is_wednesday': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': [{'color': '#1A79BB', 'polarity': 0.5, 'word': u'Many'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.5, 'word': u'more'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.4, 'word': u'confirmed'},\n",
       "  {'color': '#bb1a29', 'polarity': 0.1, 'word': u'mental'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.5, 'word': u'safe'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.5, 'word': u'most'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.4, 'word': u'important'},\n",
       "  {'color': '#bb1a29', 'polarity': 0.1, 'word': u'mental'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.13636363636363635, 'word': u'new'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.1, 'word': u'professional'},\n",
       "  {'color': '#1A79BB', 'polarity': 0.4, 'word': u'fantastic'}]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
